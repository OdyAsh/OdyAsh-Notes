
# LiteLLM

## Params for Completion Object

You can find a list of OpenAI params that LiteLLM translates across providers [here](https://docs.litellm.ai/docs/completion/input#translated-openai-params). 

Note: the params' descriptions mentioned [here](https://docs.litellm.ai/docs/completion/input#translated-openai-params) are based on OpenAI's [docs](https://platform.openai.com/docs/api-reference/chat), so they're <mark style="background: #D2B3FFA6;">not always up to date</mark>. Examples:
* `response_format` can now have `json_schema` value as well, not just `json_object`, as mentioned [here](https://platform.openai.com/docs/guides/structured-outputs/structured-outputs-vs-json-mode).
* `tools_choice` here fails to mention that you can also write `required`, as mentioned in the current OpenAI and [Groq](https://console.groq.com/docs/tool-use#tool-choice) docs.

# OpenAI API


## Format of choices Array in ModelResponse

The following is the structure of returned objects from a chat model (same as OpenAI's) ([source](https://platform.openai.com/docs/api-reference/chat/streaming)):

```python
ModelResponse(
    id='Unique identifier for the response.',
    choices=[
        StreamingChoices(
            finish_reason='The reason the model stopped generating tokens. This will be stop if the model hit a natural stop point or a provided stop sequence, length if the maximum number of tokens specified in the request was reached, content_filter if content was omitted due to a flag from our content filters, tool_calls if the model called a tool, or function_call (deprecated) if the model called a function.',
            index='The index of the choice in the list of choices.',
            delta=Delta(
                refusal='Reason for refusal, if any.',
                content='The contents of the chunk message.',
                role='The role of the author of this message chunk (user, role, or assistant).',
                function_call='###### Deprecated ###### and replaced by `tool_calls`. The name and arguments of a function that should be called, as generated by the model.',
                tool_calls=[
                    ChatCompletionDeltaToolCall(
                        id='The ID of the tool call.',
                        type='The type of the tool. As of 2024-09-01, only `function` is supported.',
                        function=Function(
                            name='Name of the function being called'
                            arguments='The arguments to call the function with, as generated by the model in JSON format. Note that the model does not always generate valid JSON, and may hallucinate parameters not defined by your function schema. Validate the arguments in your code before calling your function.',
                        ),
                        index='Index of the tool call in the response.'
                    )
                ]
            ),
            logprobs='Log probabilities of the tokens, if available.'
        )
    ],
    created='Timestamp when the response was created.',
    model='Name of the model used to generate the response.',
    object='Type of the object (e.g., chat.completion.chunk).',
    system_fingerprint='Unique fingerprint of the system.'
)
```

Note 1: `choices` is a list of chat completion choices. Can be more than one if `n` argument is greater than 1 in `litellm.get_completion()`.
* Side note: What is LiteLLM? Check [this](https://medium.com/version-1/litellm-a-comprehensive-analysis-4f0f4ede5bc8#:~:text=With%20LiteLLM%2C%20you%20can%20seamlessly%20interact%20with%20LLM%20APIs%20using%20the%20standardized%20format%20of%20OpenAI.).

Note 2: `Delta` will be replaced with `message` if `stream` argument is set to false in `litellm.get_completion()`  ([source](https://platform.openai.com/docs/api-reference/chat/object)).


## Structured Outputs vs JSON mode

 [s1](https://platform.openai.com/docs/guides/structured-outputs/structured-outputs-vs-json-mode)

| Structured Outputs     | JSON Mode                                                                                                                    |                                                   |
| ---------------------- | ---------------------------------------------------------------------------------------------------------------------------- | ------------------------------------------------- |
| **Outputs valid JSON** | Yes                                                                                                                          | Yes                                               |
| **Adheres to schema**  | Yes (see [supported schemas](https://platform.openai.com/docs/guides/structured-outputs/supported-schemas))                  | No                                                |
| **Compatible models**  | `gpt-4o-mini`, `gpt-4o-2024-08-06`, and later                                                                                | `gpt-3.5-turbo`, `gpt-4-*` and `gpt-4o-*` models  |
| **Enabling**           | response_format: { <br>  type: "json_schema", <br>  json_schema: {<br>    "strict": true, <br>    "schema": ...<br>  } <br>} | response_format: { <br>  type: "json_object"<br>} |

Regarding possible values for dictionary of `json_schema` ([s2](https://platform.openai.com/docs/api-reference/chat/create)):

* strict (optional, default: false):
	* Whether to enable strict schema adherence when generating the output. If set to true, the model will always follow the exact schema defined in the `schema` field. Only a subset of JSON Schema is supported when `strict` is `true`. To learn more, read the [Structured Outputs guide](https://platform.openai.com/docs/guides/structured-outputs).
* schema (optional):
	* The schema for the response format, described as a JSON Schema object.
* name (required):
	* The name of the response format. Must be a-z, A-Z, 0-9, or contain underscores and dashes, with a maximum length of 64.
* description (optional):
	* A description of what the response format is for, used by the model to determine how to respond in the format.


Tricky <mark style="background: #D2B3FFA6;">note</mark>: in other providers, you'll usually only see JSON mode compatibility, and not structure output compatibility. Therefore, when using frameworks like [LiteLLM](https://docs.litellm.ai/docs/completion/input#optional-fields), you most likely won't have a chance to pass `json_schema` to `response_format` parameter, and could only pass `json_object` (if that's even applicable in other providers to begin with).

## Migrating from Functions to Tools

Given that you understand the [response structure](#Format%20of%20choices%20Array%20in%20ModelResponse), and assuming you have a code like this:

```python
is_streaming = True

params = {
	'model': self.model,
	'messages': self.message_history,
	'stream': is_streaming,
	'tools': self.tools,
	'tool_choice': {'tools_choice': 'required'},
	'timeout': 30.0,
	'temperature': 0.0,
	'metadata': {'generation-name': 'app name'},
	'num_retries': 5
}
response = self.get_completion(**params)

for token in response:
	if is_streaming:
		msg_or_chnk = token.choices[0].delta # chunk of a message
	else:
		msg_or_chnk = token.choices[0].message # message

	# rest of the code
		
```

Then, in the `rest of the code` section, you'll change snippets like these:

* `if 'function_call' in msg_or_chnk` 
	* -> `if 'tool_calls' in msg_or_chnk`
* `msg_or_chnk.function_call`
	* -> `msg_or_chnk.tool_calls[0].function`
* `msg_or_chnk.function_call.arguments`
	* -> `msg_or_chnk.tool_calls[0].function.arguments`

and so on ...

# Regarding Models

## Fine-Tuned Tool Models Vs Chatbot Models Which Supports Function Calling

Be careful when reading the descriptions of models and their main goals. For example:
* suppose you want to use a model which supports [function calling](https://towardsdatascience.com/build-autonomous-ai-agents-with-function-calling-0bb483753975) 
	* So you head on to the [list of supported models](https://console.groq.com/docs/models) by a model provider (like groq), 
	* then you search for "tool" and grab the first model that you find 
	* then use it in your chatbot application.

Now, the issue in doing this is that you might have picked up a model which is fine-tuned for function calling only, and not for general chatbot interaction ([source](https://www.reddit.com/r/LocalLLaMA/comments/1e8n1y6/comment/le8apyd/?utm_source=share&utm_medium=web3x&utm_name=web3xcss&utm_term=1&utm_content=share_button)). 

Therefore, you should [look further into the model provider's documentation](https://console.groq.com/docs/tool-use#supported-models:~:text=Other%20Supported%20Models) to see models which are tailored for chatbot usage and support function calling as well.